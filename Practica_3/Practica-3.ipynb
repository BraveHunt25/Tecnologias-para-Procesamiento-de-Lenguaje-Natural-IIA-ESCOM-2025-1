{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Última modificación: 09 de noviembre de 2024_\n",
    "# Práctica 3: Identificación de frases clave y resumen automático de texto\n",
    "**Hernández Jiménez Erick Yael**: 2023630748.\n",
    "\n",
    "Escuela Superior de Cómputo.\n",
    "\n",
    "Ingeniería en inteligencia Artificial. 5BV1.\n",
    "\n",
    "Tecnologías para el Procesamiento de Lenguaje Natural.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen\n",
    "En este cuaderno se usará el procesamiento de cuerpos para realizar un resumen automático extractivo de 3 documentos tras normalizarlos. A continuación se enlistan las bibliotecas utilizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/hjey/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/hjey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk     # Biblioteca con las herramientas utilizadas para la manipulación y procesamiento de los documentos\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # Submódulo para tokenizar\n",
    "from nltk.tokenize import RegexpTokenizer  # Para generar el filtro\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')  # Lista con stopwords en distintos idiomas\n",
    "import random               # Para números aleatorios\n",
    "import math                 # Para operaciones matemáticas complejas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generación de cuerpo de documentos\n",
    "Extrayendo las primeras 3 cartas del libro de Frankstein, desde el enlace [“https://www.gutenberg.org/ebooks/84”](“https://www.gutenberg.org/ebooks/84”), los textos se guardaron manualmente como archivos de texto en la carpeta docs como [Carta_1](./docs/Carta_1.txt), [Carta_2](./docs/Carta_2.txt) y [Carta_3](./docs/Carta_3.txt). A partir de estos, incluimos un enunciado de cada uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"docs/Carta_1.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    carta_1: str = file.read()\n",
    "with open(\"docs/Carta_2.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    carta_2: str = file.read()\n",
    "with open(\"docs/Carta_3.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    carta_3: str = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizamos en enunciados para, luego, escoger un enunciado al azar de cada capítulo y así generar el documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Six years have passed since I resolved on my present undertaking. There is something at work in my soul which I do not understand. What can stop the determined heart and resolved will of man?\n"
     ]
    }
   ],
   "source": [
    "# Tokenizamos por enunciados\n",
    "original_1 = sent_tokenize(carta_1)\n",
    "original_2 = sent_tokenize(carta_2)\n",
    "original_3 = sent_tokenize(carta_3)\n",
    "\n",
    "'''\n",
    "corpus                                      Nuestra variable con el corpus\n",
    "        += str(                             Forzamos el tipo del iterable a una cadena de caracteres\n",
    "            original_x                      Para el originial número 'x'...\n",
    "                [random.randint(            escogemos un número aleatorio...\n",
    "                    0,                      entre los índices 0...\n",
    "                    len(original_x)-1)])      y la longitud de los enunciados en el original número 'x'\n",
    "        + \" \"                               Y concatenamos un espacio para distinguir entre enunciados extraídos\n",
    "'''\n",
    "corpus: str = str(original_1[random.randint(0, len(original_1)-1)]) + \" \"\n",
    "corpus += str(original_2[random.randint(0, len(original_2)-1)]) + \" \"\n",
    "corpus += str(original_3[random.randint(0, len(original_3)-1)])\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esto, tenemos el corpus sobre el que trabajaremos a lo largo de la práctica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalización de textos general\n",
    "El flujo que se usará, y su justificación se explicará a continuación:\n",
    "1. Conversión a minúsculas: para evitar redundancias en el contenido significativo del cuerpo\n",
    "2. Elimininación de espacios, números, signos de puntuación y el caracter '—': Estos caracteres se encuentra en los 3 archivos originales, siendo de carácter visual para separar diálogos y contextos en las frases. No aporta contenido al proceso de ninguno de los algoritmos por lo que su eliminación reducirá el análisis. Cabe mencionar que el caracter '—' es disntinto de '-', siendo el último relevante para la generación del resumen debido a que cambia el significado de las palabras adyacentes, por lo que se mantiene en el cuerpo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carta 1:\n",
      "six years have passed since i resolved on my present undertaking\n",
      "['six', 'years', 'have', 'passed', 'since', 'i', 'resolved', 'on', 'my', 'present', 'undertaking']\n",
      "\n",
      "Carta 2:\n",
      "there is something at work in my soul which i do not understand\n",
      "['there', 'is', 'something', 'at', 'work', 'in', 'my', 'soul', 'which', 'i', 'do', 'not', 'understand']\n",
      "\n",
      "Carta 3:\n",
      "what can stop the determined heart and resolved will of man\n",
      "['what', 'can', 'stop', 'the', 'determined', 'heart', 'and', 'resolved', 'will', 'of', 'man']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Conversión a minúsculas\n",
    "corpus = corpus.lower()\n",
    "\n",
    "# Tokenizamos por enunciados y forzamos el tipo de la varible a una lista\n",
    "documentos = sent_tokenize(corpus)\n",
    "\n",
    "# Eliminamos los caracteres que no nos interesen de cada documento sin perder la estructura u orden\n",
    "for i, doc in enumerate(documentos):\n",
    "# Para cada documento 'doc' con índice 'i' en los elementos enumerados de 'documentos'\n",
    "    documentos[i] = ''.join([char for char in doc if char.isalpha() or char == ' ' or char == '-'])\n",
    "    # Al elemento [i] lo reasignamos como la unión con el caractér vacío '' por cada caracter 'char'\n",
    "    # si 'char' es una letra o es un espacio o el guión '-'\n",
    "\n",
    "# Tokenizamos por palabras\n",
    "palabras_1 = word_tokenize(documentos[0])\n",
    "palabras_2 = word_tokenize(documentos[1])\n",
    "palabras_3 = word_tokenize(documentos[2])\n",
    "\n",
    "# Imprimimos los resultados\n",
    "print(f\"Carta 1:\\n{documentos[0]}\\n{palabras_1}\\n\\nCarta 2:\\n{documentos[1]}\\n{palabras_2}\\n\\nCarta 3:\\n{documentos[2]}\\n{palabras_3}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen automático extractivo de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF - NLTK\n",
    "Siguiendo el ejemplo de [TURING](https://www.turing.com/kb/5-powerful-text-summarization-techniques-in-python), aplicaremos el algoritmo TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crearemos clases que conserven estos datos para analizarlos posteriormente\n",
    "class TF_IDF():\n",
    "    r'''\n",
    "    Clase que contiene el resumen por TF-IDF de una serie de tokens:\n",
    "    '''\n",
    "    def __init__(self, tokens, corpus, lang: str = \"english\"):\n",
    "        r'''\n",
    "        # __init__\n",
    "        - tokens: Any = iterable con los tokens del documento sobre el que se aplicará el algoritmo TF-IDF\n",
    "        - lang: str = nombre del lenguaje a partir del cual se eliminarán las stop-words que NLTK tiene por defecto \n",
    "        '''\n",
    "        self.tokens = tokens\n",
    "        self.corpus = corpus\n",
    "        self.stopWords: set = nltk.corpus.stopwords.words(\"english\") # Definimos el conjunto de stopwords\n",
    "        self.tablaFrec: dict = {}   # Inicializamos con un diccionario vacío\n",
    "        self.pesoEnun: dict = {}    # Inicializamos con un diccionario vacío\n",
    "        self.totalPesos: int = 0    # Número descriptivo del total de los pesos\n",
    "        self.promedio: int = 0      # Número descriptivo con el promedio de los pesos\n",
    "        self.resumen: list = []     # Lista vacía para almacenar las palabras más frecuentes que conforman al resumen\n",
    "    \n",
    "    def calcularTF(self) -> None:\n",
    "        r'''\n",
    "        # generarTablaFreq\n",
    "        Genera la tabla de frecuencias normalizadas del documento\n",
    "        '''\n",
    "        # Conteo de frecuencias\n",
    "        for palabra in self.tokens:        # Por cada palabra en los tokens...\n",
    "            if palabra in self.stopWords:  # si la palabra se incluyen en las stopwords...\n",
    "                continue                # las omitimos y continuamos\n",
    "            else:                       # de lo contrario...\n",
    "                if palabra in self.tablaFrec:      # si la palabra ya se incluye en la tabla de frecuencias...\n",
    "                    self.tablaFrec[palabra] += 1   # aumentamos en 1 el contador\n",
    "                else:                           # sino...\n",
    "                    self.tablaFrec[palabra] = 1    # Empezamos el conteo en 1\n",
    "        # Normalización de frecuencias\n",
    "        for palabra in self.tablaFrec.keys():\n",
    "            self.tablaFrec[palabra] /= len(self.tokens)\n",
    "\n",
    "    def calcularIDF(self) -> None:\n",
    "        r'''\n",
    "        #calcularIDF\n",
    "        Calcular el peso que tiene cada palabra en los enunciados con respecto a todos los documentos\n",
    "        '''\n",
    "        num_docs = len(self.corpus)\n",
    "        for palabra in self.tablaFrec:\n",
    "        # Por cada palabra en la tabla de frecuencias...\n",
    "            doc_count = sum(1 for documento in self.corpus if palabra in documento)\n",
    "            # Aumentamos en uno el contador de documentos que contengan a la palabra de la iteración\n",
    "            self.pesoEnun[palabra] = math.log(num_docs / (1 + doc_count)) # Se agrega el 1 para suavizar el peso de palabras raras\n",
    "            # Para luego calcular el valor correspondiente al IDF y asignarlo al diccionario con las palabras del corpus\n",
    "\n",
    "    def imprimir_tabla(self, tabla: dict) -> None:\n",
    "        r'''\n",
    "        #imprimir_tabla\n",
    "        Imprime la tabla indicada\n",
    "        '''\n",
    "        for key in tabla:\n",
    "            print(f\"'{key}':{tabla[key]}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def calcular_TF_IDF(self) -> None:\n",
    "        self.calcularTF()\n",
    "        self.calcularIDF()\n",
    "        tf_idf = {}\n",
    "        for palabra in self.tablaFrec:\n",
    "            tf = self.tablaFrec[palabra]\n",
    "            idf = self.pesoEnun.get(palabra, 0)\n",
    "            tf_idf[palabra] = tf * idf  # Multiplicación de TF e IDF\n",
    "        self.resumen = sorted(tf_idf.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento 1\n",
      "[('six', 0.036860464373469494), ('years', 0.036860464373469494), ('passed', 0.036860464373469494), ('since', 0.036860464373469494), ('present', 0.036860464373469494)]\n",
      "Documento 2\n",
      "[('something', 0.03118962370062803), ('work', 0.03118962370062803), ('soul', 0.03118962370062803), ('understand', 0.03118962370062803)]\n",
      "Documento 3\n",
      "[('stop', 0.036860464373469494), ('determined', 0.036860464373469494), ('heart', 0.036860464373469494), ('man', 0.036860464373469494), ('resolved', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "# Aplicamos el algoritmo sobre los documentos\n",
    "td_idf_1 = TF_IDF(palabras_1, documentos)\n",
    "td_idf_2 = TF_IDF(palabras_2, documentos)\n",
    "td_idf_3 = TF_IDF(palabras_3, documentos)\n",
    "\n",
    "td_idf_1.calcular_TF_IDF()\n",
    "td_idf_2.calcular_TF_IDF()\n",
    "td_idf_3.calcular_TF_IDF()\n",
    "\n",
    "print(f\"Documento 1\\n{td_idf_1.resumen[0:5]}\")\n",
    "print(f\"Documento 2\\n{td_idf_2.resumen[0:5]}\")\n",
    "print(f\"Documento 3\\n{td_idf_3.resumen[0:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frecuencia de palabras normalizada\n",
    "De acuerdo con [Matthew Mayo](https://www.kdnuggets.com/2019/11/getting-started-automated-text-summarization.html) y [Dante Sblendorio](https://www.activestate.com/blog/how-to-do-text-summarization-with-python/). Se debe seguir el siguiente algoritmo:\n",
    "1. Normalizar los documentos.\n",
    "2.  Por cada palabra en el corpus se cuenta su frecuencia\n",
    "3.  Se obtiene la frecuencia más alta\n",
    "4.  Se divide cada frecuencia entre la frecuencia más alta\n",
    "5.  Por cada documento y cada palabra en el corpus, si la palabra se encuentra en el documento, se aumenta el conteo del enunciado\n",
    "7.  Se ordenan los enunciados por su puntación y se obtiene el resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
